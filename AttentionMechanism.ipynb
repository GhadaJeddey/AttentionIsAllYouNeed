{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "9xAgHVL2gx3E",
        "o7ln-tLtgtMw"
      ],
      "authorship_tag": "ABX9TyNv+opctinGpbuu62SaxBM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhadaJeddey/AttentionIsAllYouNeed/blob/main/AttentionMechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9T46hn30g58"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as f\n",
        "import torch.nn as nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Attention"
      ],
      "metadata": {
        "id": "0vSEzhsZcK4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAttention(nn.Module) :\n",
        "  def __init__(self,d_model,dropout=0.1):\n",
        "    super(SimpleAttention,self).__init__()\n",
        "    self.d_model = d_model # Dimension of token embeddings and attention space\n",
        "\n",
        "    self.query_layer = nn.Linear(d_model,d_model, bias = False )\n",
        "    self.key_layer = nn.Linear(d_model,d_model, bias = False)\n",
        "    self.value_layer = nn.Linear(d_model,d_model, bias = False)\n",
        "    self.dropout = nn.Dropout(dropout) # Residual Dropout page 8\n",
        "\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "\n",
        "    Q = self.query_layer(x)\n",
        "    K = self.key_layer(x)\n",
        "    V = self.value_layer(x )\n",
        "\n",
        "    batch_size, seq_length, _ = x.shape\n",
        "\n",
        "    attention_scores = torch.matmul(Q,K.transpose(-2,-1)) / (self.d_model **0.5) # matrix\n",
        "    print(attention_scores.shape)\n",
        "\n",
        "    if mask is not None :\n",
        "      mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "      mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "      attention_scores = attention_scores.masked_fill_(mask == 0,float('-inf'))\n",
        "\n",
        "    attention_weights = f.softmax(attention_scores,dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)# Residual Dropout page 8\n",
        "\n",
        "    output = torch.matmul(attention_weights, V) # outputs a tensor with context added\n",
        "\n",
        "    return attention_weights , output\n",
        "\n"
      ],
      "metadata": {
        "id": "Cqjv_9gnUU5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiHead Attention\n"
      ],
      "metadata": {
        "id": "NkxFPW9Y02rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, h, dropout = 0.1):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "\n",
        "        #model parameters\n",
        "        self.d_model = d_model # Dimension of token embeddings and attention space\n",
        "        self.h = h\n",
        "        self.d_k = d_model // self.h\n",
        "\n",
        "        #Layers\n",
        "        self.query_layer = nn.Linear(d_model,d_model, bias = False )\n",
        "        self.key_layer = nn.Linear(d_model,d_model, bias = False)\n",
        "        self.value_layer = nn.Linear(d_model,d_model, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout) # Residual Dropout page 8\n",
        "        self.projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def attention(self , Q , K , V , mask =None):\n",
        "        attention_scores= torch.matmul(Q,K.transpose(-2,-1)) / (self.d_k ** 0.5)\n",
        "        attention_weights = f.softmax(attention_scores,dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)# Residual Dropout page 8\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_weights = attention_weights.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention_weights = f.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attention_weights, V) #  shape : (batch_size, h, seq_len, d_k)\n",
        "        return output\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "      batch_size = query.size(0)\n",
        "\n",
        "      Q = self.query_layer(query).view(batch_size,  -1, self.h, self.d_k).transpose(1, 2)\n",
        "      K = self.key_layer(key).view(batch_size,  -1, self.h, self.d_k).transpose(1, 2)\n",
        "      V = self.value_layer(value ).view(batch_size,  -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "      attention = self.attention(Q,K,V,mask)\n",
        "\n",
        "      #Concatenate  heads\n",
        "      attention = attention.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model) #contiguous() ensures the tensor is laid out correctly in memory before reshaping.\n",
        "                                                                                                # PyTorch requires this when you .view() after a .transpose().\n",
        "      output = self.projection(attention) # Allows the model to learn how to weight the combined head outputs\n",
        "\n",
        "      return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yDfFmEbI1BzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "N71mHTYILUUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "YUxpoxQSWzxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape : (max_lem,1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # even indices with sine, odd indices with cosine\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # 0::2 -> start at 0 with step =2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # 1::2 -> start at 1 with step =2\n",
        "\n",
        "        # Add batch dimension\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  #registers the positional encoding as a buffer, so it won’t be updated during training.(during back prop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "z_qXFA_9LXNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Block"
      ],
      "metadata": {
        "id": "keoOeiaUpW4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads,dropout=0.1,use_ffn=True ,mask=None):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = nn.LayerNorm(d_model)\n",
        "        self.use_ffn = use_ffn\n",
        "\n",
        "        if self.use_ffn :\n",
        "          self.ffn = nn.Sequential(\n",
        "              nn.Linear(d_model, 4 * d_model , bias =True),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(4 * d_model, d_model , bias =True)\n",
        "          )\n",
        "          self.addnorm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output=None, src_mask=None, tgt_mask=None):\n",
        "\n",
        "        # Self attention\n",
        "        if enc_output is None:\n",
        "            attn_output = self.attention(x, x, x, tgt_mask)\n",
        "            x = self.addnorm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        else:\n",
        "            attn_output = self.attention(x, enc_output, enc_output, src_mask)\n",
        "\n",
        "        x = self.addnorm1(x + self.dropout(attn_output))\n",
        "\n",
        "        if self.use_ffn:\n",
        "            ffn_output = self.ffn(x)\n",
        "            x = self.addnorm2(x + ffn_output)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "O3V6e7azpWfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "fRxEWbgqvNHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 num_layers,\n",
        "                 num_embeddings,\n",
        "                 dropout=0.1,\n",
        "                 max_len=5000):\n",
        "\n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.pe = PositionalEncoding(d_model, dropout , max_len)\n",
        "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads,dropout=0.1,use_ffn=True,mask=None) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x,mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model) #Helps stabilize gradients by scaling embeddings\n",
        "        x = self.dropout(self.pe(x))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, tgt_mask=mask)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "cvAwytR-vMf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "rp-JPaGJymIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,num_embeddings, d_model, num_heads, num_layers, dropout=0.1, max_len=5000):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
        "        self.pe = PositionalEncoding(d_model, dropout, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                TransformerBlock(d_model, num_heads,dropout=0.1,use_ffn=False,mask=None),\n",
        "                TransformerBlock(d_model, num_heads,dropout=0.1,use_ffn=True,mask=None)\n",
        "\n",
        "            ])\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.linear = nn.Linear(d_model, num_embeddings)\n",
        "\n",
        "    def forward(self, x,enc_output, src_mask=None, tgt_mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        for block_pair in self.layers:\n",
        "            x = block_pair[0](x, enc_output, src_mask, tgt_mask)  # without FFN\n",
        "            x = block_pair[1](x, enc_output, src_mask, tgt_mask)  # with FFN\n",
        "\n",
        "        return self.linear(x)\n"
      ],
      "metadata": {
        "id": "EYUYGzZ-yu1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "PtGQXDsf9Fpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self , num_embeddings, d_model=512, num_heads=8 , num_layers=6, dropout=0.1, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(d_model, num_heads, num_layers, num_embeddings,dropout, max_len)\n",
        "        self.decoder = Decoder(num_embeddings, d_model, num_heads, num_layers, dropout, max_len)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "-Q4c0GB69JkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing"
      ],
      "metadata": {
        "id": "KhHYWnJg4qNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install py7zr\n",
        "!pip install tokenizers\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E79Y0ABtVmgx",
        "outputId": "fb357d54-e11c-4023-909e-9c74ef6a982b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.22.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.11/dist-packages (from py7zr) (0.16.2)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.1.1)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.0.3)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.0.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (5.9.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "9xAgHVL2gx3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qXBtOHLU5Co",
        "outputId": "c83874fb-39b7-4238-9412-318f932e791e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "val_dataset = dataset[\"validation\"]"
      ],
      "metadata": {
        "id": "ErP8MyV1VDws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "o7ln-tLtgtMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [item['article'] + '' + item['highlights'] for item in train_dataset ]\n",
        "with open('train.txt', 'w',encoding='utf-8') as f:\n",
        "    for text in texts:\n",
        "        f.write(text.strip() + '\\n')"
      ],
      "metadata": {
        "id": "tB4BsZJNWzOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=30000, special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]):\n",
        "        self.special_tokens = special_tokens\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        self.tokenizer.pre_tokenizer = Whitespace()\n",
        "        self.trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "\n",
        "        # Will be set after training or loading\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.pad_id = None\n",
        "\n",
        "    def train(self, files, save_path=\"my_tokenizer.json\"):\n",
        "        self.tokenizer.train(files, self.trainer)\n",
        "        self.tokenizer.save(save_path)\n",
        "        print(f\"Tokenizer saved to {save_path}\")\n",
        "        self._set_pad_id()\n",
        "\n",
        "    def load(self, path=\"my_tokenizer.json\"):\n",
        "        self.tokenizer = Tokenizer.from_file(path)\n",
        "        print(f\"Tokenizer loaded from {path}\")\n",
        "        self._set_pad_id()\n",
        "\n",
        "    def _set_pad_id(self):\n",
        "        # Get pad token ID from vocab\n",
        "        self.pad_id = self.tokenizer.token_to_id(self.pad_token)\n",
        "        if self.pad_id is None:\n",
        "            raise ValueError(f\"{self.pad_token} not found in tokenizer vocabulary!\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.tokenizer.encode(text).ids\n",
        "\n",
        "    def batch_encode(self, texts):\n",
        "        return [self.encode(t) for t in texts]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.tokenizer.decode(ids)\n",
        "\n",
        "    def save(self, path=\"my_tokenizer.json\"):\n",
        "        self.tokenizer.save(path)\n",
        "        print(f\"Tokenizer saved to {path}\")\n",
        "\n",
        "    def collate_fn(self, batch): # pytorch func\n",
        "\n",
        "        if self.pad_id is None:\n",
        "            self._set_pad_id()\n",
        "\n",
        "        if isinstance(batch[0], tuple):\n",
        "            texts, labels = zip(*batch)\n",
        "            encoded = [torch.tensor(self.encode(text), dtype=torch.long) for text in texts]\n",
        "            padded = pad_sequence(encoded, batch_first=True, padding_value=self.pad_id)\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "            return padded, labels\n",
        "        else:\n",
        "            encoded = [torch.tensor(self.encode(text), dtype=torch.long) for text in batch]\n",
        "            padded = pad_sequence(encoded, batch_first=True, padding_value=self.pad_id)\n",
        "            return padded\n"
      ],
      "metadata": {
        "id": "rFWqL5z_iNe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcf4dff-4400-4880-a6c1-11637540193d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer = BPETokenizer()\n",
        "bpe_tokenizer.train([\"train.txt\"])\n",
        "bpe_tokenizer.save(\"/content/drive/MyDrive/AttentionIsAllYouNeed/my_tokenizer.json\")"
      ],
      "metadata": {
        "id": "gPzMTwIM0dJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer = BPETokenizer()\n",
        "bpe_tokenizer.load(\"/content/drive/MyDrive/AttentionIsAllYouNeed/my_tokenizer.json\")"
      ],
      "metadata": {
        "id": "lkxIJVO_SY7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "b6739896-79db-4cdd-dba6-7da8d759f13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BPETokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f95090708157>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbpe_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPETokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbpe_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/AttentionIsAllYouNeed/my_tokenizer.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BPETokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de texte à tester\n",
        "texts = [\"Hello, how are you?\", \"I am fine, thank you!\", \"What about you?\"]\n",
        "\n",
        "# Tester la tokenisation\n",
        "encoded_texts = [bpe_tokenizer.encode(text) for text in texts]\n",
        "print(\"Tokenized texts:\")\n",
        "for text, encoded in zip(texts, encoded_texts):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Encoded: {encoded}\")\n",
        "\n",
        "# Tester le padding\n",
        "padded_batch = bpe_tokenizer.collate_fn(texts)\n",
        "print(\"\\nPadded batch:\")\n",
        "print(padded_batch)\n",
        "\n",
        "decoded_texts = [bpe_tokenizer.decode(encoded) for encoded in encoded_texts]\n",
        "print(\"\\nDecoded texts:\")\n",
        "for decoded in decoded_texts:\n",
        "    print(decoded)"
      ],
      "metadata": {
        "id": "xiaKO3MznQda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0088a08-6e11-43dc-82de-fde35139c997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized texts:\n",
            "Text: Hello, how are you?\n",
            "Encoded: [17241, 15, 928, 565, 740, 34]\n",
            "Text: I am fine, thank you!\n",
            "Encoded: [44, 537, 3520, 15, 5560, 740, 4]\n",
            "Text: What about you?\n",
            "Encoded: [2060, 728, 740, 34]\n",
            "\n",
            "Padded batch:\n",
            "tensor([[17241,    15,   928,   565,   740,    34,     0],\n",
            "        [   44,   537,  3520,    15,  5560,   740,     4],\n",
            "        [ 2060,   728,   740,    34,     0,     0,     0]])\n",
            "\n",
            "Decoded texts:\n",
            "Hello , how are you ?\n",
            "I am fine , thank you !\n",
            "What about you ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Transformer"
      ],
      "metadata": {
        "id": "0Qi7zvcqhsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "texts = [example['article'] for example in train_dataset]\n",
        "summaries = [example['highlights'] for example in train_dataset]\n",
        "\n",
        "train_encodings = bpe_tokenizer.batch_encode(texts)\n",
        "summarized_encodings = bpe_tokenizer.batch_encode(summaries)\n",
        "## save to drive\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AttentionIsAllYouNeed/train_encodings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_encodings, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AttentionIsAllYouNeed/summary_encodings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(summarized_encodings, f)\n"
      ],
      "metadata": {
        "id": "36abVGzwjwBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/AttentionIsAllYouNeed/train_encodings.pkl\", \"rb\") as f:\n",
        "    train_encodings = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/AttentionIsAllYouNeed/summary_encodings.pkl\", \"rb\") as f:\n",
        "    summarized_encodings = pickle.load(f)"
      ],
      "metadata": {
        "id": "wSoiMTSASkBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON-m_eMiTBUQ",
        "outputId": "4058084c-96c8-40db-d11e-9bacca43d5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Hyperparameters for the transformer model\n",
        "vocab_size = bpe_tokenizer.tokenizer.get_vocab_size()\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_layers = 6\n",
        "\n",
        "model = Transformer(\n",
        "    num_embeddings=vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=nhead,\n",
        "    num_layers=num_layers,\n",
        "    dropout=0.1,\n",
        "    max_len=5000\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=bpe_tokenizer.tokenizer.token_to_id(\"<pad>\"))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=bpe_tokenizer.tokenizer.token_to_id(\"<pad>\"))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_encodings), 16):  # Batch size 16\n",
        "        batch_texts = train_encodings[i:i+16]\n",
        "        batch_summaries = summarized_encodings[i:i+16]\n",
        "\n",
        "        # Pad and convert to tensor\n",
        "        src = pad_sequence([torch.tensor(x) for x in batch_texts], batch_first=True, padding_value=bpe_tokenizer.pad_id).to(device)\n",
        "        tgt = pad_sequence([torch.tensor(x) for x in batch_summaries], batch_first=True, padding_value=bpe_tokenizer.pad_id).to(device)\n",
        "\n",
        "        output = model(src, tgt[:, :-1])  # Exclude last token in decoder input\n",
        "        loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].reshape(-1))  # Shifted target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / (len(train_encodings) // 16)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "coTAhcjM2k5T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e2a23a32-91bf-4a67-fa80-5c7a04327414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'bpe_tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c9514034689>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Hyperparameters for the transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpe_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bpe_tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "0aeKAa7O4piP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Example for generating summaries on the validation set\n",
        "generated_summaries = []\n",
        "for i in range(len(val_dataset)):\n",
        "    input_text = val_dataset[i]['article']\n",
        "    input_ids = bpe_tokenizer.encode(input_text)\n",
        "    output = model(input_ids, input_ids)\n",
        "    generated_summaries.append(bpe_tokenizer.decode(output.argmax(dim=-1)))\n",
        "\n",
        "# Here, you would calculate ROUGE score or any other metric to evaluate the model\n",
        "# For example, using the rouge-score package:\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Calculate the ROUGE score between generated summaries and real summaries\n",
        "for i in range(len(generated_summaries)):\n",
        "    scorer.add_summary(generated_summaries[i], val_dataset[i]['highlights'])\n",
        "\n",
        "print(f\"ROUGE score: {scorer}\")\n"
      ],
      "metadata": {
        "id": "r-DK7J-X4lLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}